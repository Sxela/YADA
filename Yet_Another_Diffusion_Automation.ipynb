{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Yet Another Diffusion Automation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "YADA v0.1.0 by [me](https://github.com/Sxela) A minimal colab with support for init images, clip and lpips losses.\n",
        "\n",
        "While we're all waiting for stable diffusion weights to go public, it's also fun to try running ye olde faithful latent diffusion model with the same settings and modes to get ready for day 1 :D\n",
        "\n",
        "This is a fun project, don't take it seriously. Best, [Alex](https://twitter.com/devdef).\n",
        "\n",
        "As usual, kudos to [Katherine Crowson](https://github.com/crowsonkb) for sticking all the fancy samplers in one [place](https://github.com/crowsonkb/k-diffusion)\n",
        "and to [DamascusGit](https://github.com/DamascusGit) for pointers and the idea"
      ],
      "metadata": {
        "id": "ksC0jyia55Sl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cu0CKPi5v3cg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7332343-64c7-4630-ef2b-ead90e684f85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Aug 21 19:41:20 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    22W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOwf8EuLwUzN",
        "outputId": "bdc01dbd-cfe7-4147-a462-6698b6431495"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try: \n",
        "  import k_diffusion as K\n",
        "except: \n",
        "  !mkdir -p models/ldm/txt2img_f8_large/\n",
        "  !wget -O models/ldm/txt2img_f8_large/model.ckpt https://ommer-lab.com/files/latent-diffusion/nitro/txt2img-f8-large/model.ckpt\n",
        "  !git clone https://github.com/CompVis/stable-diffusion\n",
        "  !pip install -e /content/stable-diffusion\n",
        "  !pip install ipywidgets==7.7.1\n",
        "  !pip install transformers==4.19.2\n",
        "\n",
        "  !pip install omegaconf\n",
        "  !pip install einops\n",
        "  !pip install pytorch_lightning\n",
        "  !pip install scikit-image\n",
        "  !pip install opencv-python\n",
        "  !pip install ai-tools\n",
        "  !pip install cognitive-face\n",
        "  !pip install zprint\n",
        "  !pip install kornia\n",
        "\n",
        "  !pip install -e git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers\n",
        "  !pip install -e git+https://github.com/openai/CLIP.git@main#egg=clip\n",
        "\n",
        "  %pip install lpips\n",
        "  %pip install keras\n",
        "\n",
        "  %cd /content/\n",
        "  !git clone https://github.com/crowsonkb/k-diffusion/\n",
        "  %cd k-diffusion\n",
        "  !pip install -e .\n",
        "\n",
        "  !pip uninstall pillow -y\n",
        "  !pip install pillow"
      ],
      "metadata": {
        "id": "224SASAAwaN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Restart session after installing, because some package here occasionally (or not) kills PIL"
      ],
      "metadata": {
        "id": "8ed4stbg6xIB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# k-diffusion"
      ],
      "metadata": {
        "id": "grOSXuhbyeb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/k-diffusion\n",
        "import argparse\n",
        "import math,os,time\n",
        "\n",
        "import accelerate\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm.notebook import trange, tqdm\n",
        "\n",
        "import k_diffusion as K\n",
        "from pytorch_lightning import seed_everything\n",
        "\n",
        "from omegaconf import OmegaConf\n",
        "from ldm.util import instantiate_from_config\n",
        "\n",
        "from torch import autocast\n",
        "import numpy as np\n",
        "\n",
        "from einops import rearrange\n",
        "from torchvision.utils import make_grid\n",
        "import PIL.Image\n",
        "from torchvision import transforms"
      ],
      "metadata": {
        "id": "ovJaPAGhyg62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CFGDenoiser(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.inner_model = model\n",
        "\n",
        "    def forward(self, x, sigma, uncond, cond, cond_scale):\n",
        "        x_in = torch.cat([x] * 2)\n",
        "        sigma_in = torch.cat([sigma] * 2)\n",
        "        cond_in = torch.cat([uncond, cond])\n",
        "        uncond, cond = self.inner_model(x_in, sigma_in, cond=cond_in).chunk(2)\n",
        "        return uncond + (cond - uncond) * cond_scale\n",
        "        \n",
        "import gc \n",
        "def load_model_from_config(config, ckpt, verbose=False):\n",
        "    print(f\"Loading model from {ckpt}\")\n",
        "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
        "    if \"global_step\" in pl_sd:\n",
        "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
        "    sd = pl_sd[\"state_dict\"]\n",
        "    del pl_sd\n",
        "    gc.collect()\n",
        "    model = instantiate_from_config(config.model)\n",
        "    m, u = model.load_state_dict(sd, strict=False)\n",
        "    if len(m) > 0 and verbose:\n",
        "        print(\"missing keys:\")\n",
        "        print(m)\n",
        "    if len(u) > 0 and verbose:\n",
        "        print(\"unexpected keys:\")\n",
        "        print(u)\n",
        "\n",
        "    model.cuda().half()\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "config = OmegaConf.load(f\"/content/stable-diffusion/configs/latent-diffusion/txt2img-1p4B-eval.yaml\")\n",
        "model = load_model_from_config(config, f\"/content/models/ldm/txt2img_f8_large/model.ckpt\")"
      ],
      "metadata": {
        "id": "v077DGFH_0ZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Settings"
      ],
      "metadata": {
        "id": "7KTgrgyi8ItK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "\n",
        "\n",
        "opt = AttrDict(\n",
        "seed = 1134143,\n",
        "batch_size = 1,\n",
        "outdir = './outputs',\n",
        "n_rows = 3,\n",
        "n_iter = 2,\n",
        "scale = 10,\n",
        "C = 16, #4\n",
        "f = 16, #16\n",
        "H = 512 ,\n",
        "W = 512,\n",
        "fixed_code = False,\n",
        "prompt = 'a gta 5 loading screen featuring snoop dogg',\n",
        "ddim_steps = 100,\n",
        "clip_guidance_scale = 200., #200\n",
        "init_latent_scale = 0., #20\n",
        "clip_type = 'ViT-L/14@336px',\n",
        "prompt_clip = '',\n",
        "init_vgg_scale = 0, #1000\n",
        "clamp_grad = True,\n",
        "clamp_max = 0.7,\n",
        "strength = 1, #1 - ninit image, no skip steps\n",
        "init_image = '/content/2fdaaee2-7a80-42cc-a524-59838ff40128_32.png',\n",
        "dynamic_thresh = 2.\n",
        ")"
      ],
      "metadata": {
        "id": "wO6PXsPS8LCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if opt.prompt_clip == '':   opt.prompt_clip = opt.prompt\n",
        "\n",
        "seed = opt.seed\n",
        "batch_size = opt.batch_size\n",
        "outdir = opt.outdir\n",
        "n_rows = opt.n_rows\n",
        "n_iter = opt.n_iter\n",
        "scale = opt.scale\n",
        "C = opt.C\n",
        "f = opt.f\n",
        "H = opt.H\n",
        "W = opt.W\n",
        "fixed_code = opt.fixed_code\n",
        "prompt = opt.prompt\n",
        "ddim_steps = opt.ddim_steps\n",
        "clip_guidance_scale =opt.clip_guidance_scale\n",
        "init_latent_scale = opt.init_latent_scale\n",
        "clip_type = opt.clip_type\n",
        "prompt_clip = opt.prompt_clip\n",
        "init_vgg_scale = opt.init_vgg_scale\n",
        "\n",
        "\n",
        "accelerator = accelerate.Accelerator()\n",
        "device = accelerator.device\n",
        "seed_everything(seed)\n",
        "seeds = torch.randint(-2 ** 63, 2 ** 63 - 1, [accelerator.num_processes])\n",
        "torch.manual_seed(seeds[accelerator.process_index].item())\n",
        "\n",
        "os.makedirs(outdir, exist_ok=True)\n",
        "outpath = outdir\n",
        "\n",
        "n_rows =  n_rows if  n_rows > 0 else batch_size\n",
        "sample_path = os.path.join(outpath, \"samples\")\n",
        "os.makedirs(sample_path, exist_ok=True)\n",
        "base_count = len(os.listdir(sample_path))\n",
        "grid_count = len(os.listdir(outpath)) - 1\n",
        "\n",
        "start_code = None\n",
        "if fixed_code:\n",
        "        start_code = torch.randn([batch_size, C, H // f, W // f], device=device)\n",
        "precision_scope = autocast\n",
        "\n",
        "\n",
        "assert prompt is not None\n",
        "data = [batch_size * [prompt]]\n",
        "\n",
        "assert 0. <= opt.strength <= 1., 'can only work with strength in [0.0, 1.0]'\n",
        "t_enc = int(opt.strength * opt.ddim_steps)\n",
        "print(f\"target t_enc is {t_enc} steps\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZHgYbmwAx3a",
        "outputId": "1fddfff2-6a30-4729-d0cf-fd10f298545c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.seed:Global seed set to 1134143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "target t_enc is 100 steps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import clip\n",
        "from kornia import augmentation as KA\n",
        "from torch.nn import functional as F\n",
        "from resize_right import resize\n",
        "\n",
        "def spherical_dist_loss(x, y):\n",
        "    x = F.normalize(x, dim=-1)\n",
        "    y = F.normalize(y, dim=-1)\n",
        "    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)\n",
        "\n",
        "\n",
        "clip_model = clip.load(clip_type, device=device)[0].eval().requires_grad_(False)\n",
        "clip_normalize = transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073),\n",
        "                                          std=(0.26862954, 0.26130258, 0.27577711))\n",
        "clip_size = (clip_model.visual.input_resolution, clip_model.visual.input_resolution)\n",
        "aug = KA.RandomAffine(0, (1/14, 1/14), p=1, padding_mode='border')\n"
      ],
      "metadata": {
        "id": "cnA9o--hUOmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_wrap = K.external.CompVisDenoiser(model)\n",
        "sigma_min, sigma_max = model_wrap.sigmas[0].item(), model_wrap.sigmas[-1].item()"
      ],
      "metadata": {
        "id": "XVm8VxE4avi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from einops import rearrange, repeat\n",
        "\n",
        "def make_cond_model_fn(model, cond_fn):\n",
        "    def model_fn(x, sigma, **kwargs):\n",
        "        with torch.enable_grad():\n",
        "            x = x.detach().requires_grad_()\n",
        "            denoised = model(x, sigma, **kwargs);# print(denoised.requires_grad)\n",
        "            \n",
        "            cond_grad = cond_fn(x, sigma, denoised=denoised, **kwargs).detach();# print(cond_grad.requires_grad)\n",
        "            cond_denoised = denoised.detach() + cond_grad * K.utils.append_dims(sigma**2, x.ndim)\n",
        "        return cond_denoised\n",
        "    return model_fn\n",
        "\n",
        "\n",
        "def make_static_thresh_model_fn(model, value=1.):\n",
        "    def model_fn(x, sigma, **kwargs):\n",
        "        return model(x, sigma, **kwargs).clamp(-value, value)\n",
        "    return model_fn\n",
        "\n",
        "def get_image_embed(x):\n",
        "        if x.shape[2:4] != clip_size:\n",
        "            x = resize(x, out_shape=clip_size, pad_mode='reflect')\n",
        "        x = clip_normalize(x)\n",
        "        x = clip_model.encode_image(x).float()\n",
        "        return F.normalize(x)\n",
        "\n",
        "def load_img(path):\n",
        "    image = PIL.Image.open(path).convert(\"RGB\")\n",
        "    w, h = image.size\n",
        "    print(f\"loaded input image of size ({w}, {h}) from {path}\")\n",
        "    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n",
        "    image = image.resize((opt.W, opt.H), resample=PIL.Image.LANCZOS)\n",
        "    image = np.array(image).astype(np.float32) / 255.0\n",
        "    image = image[None].transpose(0, 3, 1, 2)\n",
        "    image = torch.from_numpy(image)\n",
        "    return 2.*image - 1.\n",
        "\n",
        "init_image = opt.init_image\n",
        "init_image = load_img(init_image).to(device)\n",
        "init_image = repeat(init_image, '1 ... -> b ...', b=batch_size).half()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "target_embed = F.normalize(clip_model.encode_text(clip.tokenize(prompt_clip, truncate=True).to(device)).float())\n",
        "init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image)) \n",
        "x0 = init_latent\n",
        "\n",
        "def cond_fn(x, t, denoised, **kwargs): #compare text clip embeds with denoised image embeds\n",
        "        denoised = model_wrap_cfg.inner_model.inner_model.decode_first_stage(denoised);# print(denoised.requires_grad)\n",
        "        image_embed = get_image_embed(aug(denoised.add(1).div(2)))\n",
        "        loss = spherical_dist_loss(image_embed, target_embed).sum() * clip_guidance_scale\n",
        "        grad = -torch.autograd.grad(loss, x)[0]\n",
        "        return grad\n",
        "\n",
        "def cond_fn(x, t, denoised, **kwargs): #compare init image latent with denoised latent \n",
        "        loss = spherical_dist_loss(denoised, init_latent).sum() * init_latent_scale\n",
        "        print(loss)\n",
        "        grad = -torch.autograd.grad(loss, x)[0]\n",
        "        return grad\n",
        "\n",
        "import lpips\n",
        "lpips_model = lpips.LPIPS(net='vgg').to(device)\n",
        "\n",
        "def cond_fn(x, t, denoised, **kwargs): \n",
        "        loss = 0. \n",
        "        grad = torch.zeros_like(x)\n",
        "        if clip_guidance_scale>0:\n",
        "          #compare text clip embeds with denoised image embeds\n",
        "          denoised = model_wrap_cfg.inner_model.inner_model.differentiable_decode_first_stage(denoised);# print(denoised.requires_grad)\n",
        "          image_embed = get_image_embed(aug(denoised.add(1).div(2)))\n",
        "          loss = spherical_dist_loss(image_embed, target_embed).sum() * clip_guidance_scale\n",
        "       \n",
        "        if init_latent_scale>0:\n",
        "          #compare init image latent with denoised latent \n",
        "          loss += spherical_dist_loss(denoised, init_latent).sum() * init_latent_scale\n",
        "        \n",
        "        if init_vgg_scale>0:\n",
        "          #compare init image with denoised latent image via lpips\n",
        "          denoised = model_wrap_cfg.inner_model.inner_model.differentiable_decode_first_stage(denoised)\n",
        "          loss += lpips_model(denoised, init_image).sum() * init_vgg_scale\n",
        "          \n",
        "        if loss!=0. :\n",
        "          grad = -torch.autograd.grad(loss, x)[0]\n",
        "          # print(loss, grad.max())\n",
        "          if opt.clamp_grad:\n",
        "            magnitude = grad.square().mean().sqrt()\n",
        "            return grad * magnitude.clamp(max=opt.clamp_max) / magnitude\n",
        "        return grad\n",
        "\n",
        "model_wrap_cfg = CFGDenoiser(model_wrap)\n",
        "model_fn = make_cond_model_fn(model_wrap_cfg, cond_fn)\n",
        "model_fn = make_static_thresh_model_fn(model_fn, opt.dynamic_thresh)"
      ],
      "metadata": {
        "id": "UXrMDWAkaQhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_everything(seed)\n",
        "with torch.no_grad():\n",
        "        with precision_scope(\"cuda\"):\n",
        "            with model.ema_scope():\n",
        "                tic = time.time()\n",
        "                all_samples = list()\n",
        "                for n in trange(n_iter, desc=\"Sampling\", disable =not accelerator.is_main_process):\n",
        "                    for prompts in tqdm(data, desc=\"data\", disable=not accelerator.is_main_process):\n",
        "                        uc = None\n",
        "                        if scale != 1.0:\n",
        "                            uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
        "                            \n",
        "                        if isinstance(prompts, tuple):\n",
        "                            prompts = list(prompts)\n",
        "                        c = model.get_learned_conditioning(prompts)\n",
        "                        \n",
        "                        shape = [C, H // f, W // f]\n",
        "                        sigmas = model_wrap.get_sigmas(ddim_steps)\n",
        "                        extra_args = {'cond': c, 'uncond': uc, 'cond_scale': scale}\n",
        "                        if opt.strength<1.0:\n",
        "                          noise = torch.randn_like(x0) * sigmas[opt.ddim_steps - t_enc - 1] # for GPU draw\n",
        "                          xi = x0 + noise\n",
        "                          sigma_sched = sigmas[opt.ddim_steps - t_enc - 1:]\n",
        "                          samples_ddim = K.sampling.sample_lms(model_fn, xi, sigma_sched, extra_args=extra_args, disable=not accelerator.is_main_process)\n",
        "                        else: \n",
        "                          x = torch.randn([batch_size, *shape], device=device) * sigmas[0]\n",
        "                          samples_ddim = K.sampling.sample_lms(model_fn, x, sigmas, extra_args=extra_args, disable=not accelerator.is_main_process)\n",
        "\n",
        "                        x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
        "                        x_samples_ddim = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "                        x_samples_ddim = accelerator.gather(x_samples_ddim)        \n",
        "\n",
        "                        if accelerator.is_main_process:\n",
        "                            for x_sample in x_samples_ddim:\n",
        "                                x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
        "                                PIL.Image.fromarray(x_sample.astype(np.uint8)).save(\n",
        "                                    os.path.join(sample_path, f\"{base_count:05}.png\"))\n",
        "                                base_count += 1\n",
        "\n",
        "                        if accelerator.is_main_process:\n",
        "                            all_samples.append(x_samples_ddim)\n",
        "\n",
        "                if accelerator.is_main_process:\n",
        "                    # additionally, save as grid\n",
        "                    grid = torch.stack(all_samples, 0)\n",
        "                    grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
        "                    grid = make_grid(grid, nrow=n_rows)\n",
        "\n",
        "                    # to image\n",
        "                    grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
        "                    with open(os.path.join(outpath, f'grid-{grid_count:04}.txt'), \"w\") as text_file:\n",
        "                        print(opt, file=text_file)\n",
        "                    PIL.Image.fromarray(grid.astype(np.uint8)).save(os.path.join(outpath, f'grid-{grid_count:04}.png'))\n",
        "                    grid_count += 1\n",
        "\n",
        "                toc = time.time()\n",
        "\n",
        "print(f\"Your samples are ready and waiting for you here: \\n{outpath} \\n\"\n",
        "          f\" \\nEnjoy.\")                \n"
      ],
      "metadata": {
        "id": "g2yfk86HPHeu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}